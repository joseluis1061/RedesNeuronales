{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de clientes bancarios\n",
    "\n",
    "Esta es la segunda versión del modelo **RNA** usando **K-folds cross validation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sesgo y varianza\n",
    "**Sesgo:** El sesgo se define como la diferencia entre el valor real y, y el valor calculado y_hat en el **entrenamiento**. Esta medida esta inversamente relacionada con la complejidad de los modelos. A mayor complejidad del modelo, obtendremos un bajo sesgo. En el caso de redes neuronales aumentar la complejidad esta relasionado con aumentar el número de capas y la cantidad de neuronas dentro de cada capa. Se debe recordar que a medida que aumentamos la complejidad incurriremos en overfitting. Si lo analisas de manera mas consiente tiene logica, pues al sobre entrenar el modelo, las predicciones seran exactamente o casi las mismas medidas reales. Una condición no deseada. \n",
    "\n",
    "![Overfit and underfit](./img/Overfit.bmp)\n",
    "\n",
    "**Varianza:** Por otro lado la varianza se trata de la variabilidad que obtenemos en los resultados del modelo al entregarle datos no vistos o mejor conocidos como datos de **test**. Es natural que las medidas obtenidas no sean exactamente iguales con datos no vistos por el modelo, pero se espera que el modelo sea capaz de encontrar patrones que le ayuden a interpretar estos nuevos datos, entregando resulatados diferencias minimas entre el resultado real y el predicho, osea con baja varianza. Nuevamente se puede inferir que a medida que un modelo presenta un overfit o sobre entrenamiento, obtengamos datos con una varianz alta. Esto se debe a que el modelo ha memorizado los datos de entrenamiento y no sabe como responder a datos no vistos. \n",
    "\n",
    "\n",
    "**Conclusión 1:** Podemos ver entoces que si disminuimos la varianza del modelo, terminaremos tambien por minimizar el sesgo. Nuestro objetivo entonces es diminuir la varianza de nuestros modelos. \n",
    "\n",
    "![Overfit and underfit](./img/VarianzaYSesgo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-fold cross validation\n",
    "Es un metodo que nos ayuda a disminuir la varianza de nuesto sistema sin incurrir en overfiting. El metodo consiste en:\n",
    "\n",
    "1. Dividir los datos en folds o grupos, se recomienda usar entre 5-10 divisiones. \n",
    "2. En cada iteracion tomaremos un grupo de datos para usarlo como test. \n",
    "3. En cada iteración se cambia el grupo de test de tal modo que el anterior grupo que se uso para test pasara a ser de entrenamiento.\n",
    "4. De esta manera, al terminar de recorrer todas las iteraciones habremos usado todos los datos para test y para entrenamiento.\n",
    "5. Al final el algoritmo nos entregara la presición media entre todos los k-folds que implementamos. Es natural que se haga así, pues si tomamos el mejor resultado, tendriamos un sesgo en la información.\n",
    "\n",
    "De esta manera el sesgo o bias y la varianza tienden a disminuir sin caer en overfit. \n",
    "\n",
    "![Overfit and underfit](./img/k-fold.png)\n",
    "\n",
    "Como se ha mensionado en el resumen del metódo, es necesario cambiar la manera en que se dividin los datos de train. Se procedera a cambiar la versión anterior del código para implementar K-folds, esto se hace en el tramo de código que implementa el modelo, todos los procesos anteriores son validos hasta este punto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited\n",
       "0          1    15634602  Hargrave          619    France  Female   42       2       0.00              1          1               1        101348.88       1\n",
       "1          2    15647311      Hill          608     Spain  Female   41       1   83807.86              1          0               1        112542.58       0\n",
       "2          3    15619304      Onio          502    France  Female   42       8  159660.80              3          1               0        113931.57       1\n",
       "3          4    15701354      Boni          699    France  Female   39       1       0.00              2          0               0         93826.63       0\n",
       "4          5    15737888  Mitchell          850     Spain  Female   43       2  125510.82              1          1               1         79084.10       0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cargar los datos\n",
    "data = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = (10000, 10) y= (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Separar variables dependientes e independientes\n",
    "X = data.iloc[:,3:13].values\n",
    "y = data.iloc[:,13].values\n",
    "print(f'X = {X.shape} y= {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decodificar datos categoricos\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Hacemos una variable dummy para la columna 1 de paises\n",
    "ct = ColumnTransformer([(\"Geography\", OneHotEncoder(), [1])], remainder = 'passthrough')\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# Y una variable binaria o de nivel para la columna 2 genero de los clientes\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 4] = labelencoder_X.fit_transform(X[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 11)\n"
     ]
    }
   ],
   "source": [
    "#Eliminar una columna de variables dummy para evitar la multicolinealidad\n",
    "X = X[:,1:]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (8000, 11), X_test(2000, 11), y_train(8000,), y_test(2000,)\n"
     ]
    }
   ],
   "source": [
    "#Dividir datos en train y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "print(f'X_train {X_train.shape}, X_test{X_test.shape}, y_train{y_train.shape}, y_test{y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizar los datos de train\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Creamos un objeto base para escalar las variables\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "#Usamos la misma base de escala para los datos de test pues pertenecen al mismo grupo\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias\n",
    "1. Se debe notar que usaremos wrappers o fragmentos de código de scikit_learn pero implementados en keras, osea es una combinación de las dos librerias. \n",
    "2. cross_val_score implementa K-folds y sera el encargado de ejecutar el entrenamiento de los datos. \n",
    "3. Es necesario definir nuestro modelo como una función a la cual llamaremos para implementar el nuevo metódo de train y test.\n",
    "4. El encargado de hacer este proceso es el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construir la red neuronal artificial RNA\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Función para construir nuestro modelo\n",
    "def model_clasificator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 6, kernel_initializer = \"uniform\", activation = \"relu\", input_dim = 11))\n",
    "    model.add(Dense(units=6, kernel_initializer = 'uniform', activation='relu'))\n",
    "    #Agregar la capa de salida\n",
    "    model.add(Dense(units=1, kernel_initializer = 'uniform', activation='sigmoid'))\n",
    "    #Mostrar un resumen del modelo \n",
    "    model.summary()\n",
    "    #Compilar el modelo\n",
    "    model.compile(optimizer = \"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***KerasClassifier*** requiere como argumentos:\n",
    "- build_fn = función con nuestro modelo compilado\n",
    "- Parametros de sklearn: Como se indico es un wrapper, el cual contiene la libreria de sklearn, por lo tanto, usaremos los parametros del .fit de esta libreria, epochs, batch_size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invocaremos el modelo mediante una variable clasifier\n",
    "clasifier = KerasClassifier(build_fn = model_clasificator, epochs = 100, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***cross_val_score*** este proceso retorna varias medidas de los resultados de nuestro modelo, en este caso [accuracy] de cada iteración, por lo tanto es necesario guardar esto datos en un array. Los argumentos para esta función son:\n",
    "1. estimator = objeto que usaremos para medir las presiciones obtenidos en este caso el clasifier\n",
    "2. X = Conjunto de datos original que usaremos para entrenamiento del cual sacaremos un porcentaje para validar en cada iteración. \n",
    "3. y = Conjunto de datos con etiquetas de train\n",
    "4. cv = número de folds para hacer la cross validation se recomienda 5 - 10. Se pueden superar este valor de 10 si tienes una maquina potente. \n",
    "5. n_jobs = Si deseas usar todos los core de tu máquina para resolver el ajuste de modelos pues escribir -1 en este parametro.\n",
    "6. verbose = 1 para motrar los resultados de cada ajuste 0 para ocultarlos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  2.8min remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.4min finished\n"
     ]
    }
   ],
   "source": [
    "accurasies = cross_val_score(estimator = clasifier, X = X_train, y = y_train, cv = 10, n_jobs = -1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85624999, 0.85000002, 0.83625001, 0.83499998, 0.85250002,\n",
       "       0.85374999, 0.83375001, 0.83499998, 0.81      , 0.84875   ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluar las presiciones obtenidas con el metodo de validación cruzada\n",
    "accurasies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediremos cual es la media de la presición obtenida y la varainza, de esta manera podemos determinar si son altas o bajas y podremos situarnos en alguna de las cuatro posibilidades vistas en el apartado inicial del código. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varianza = 0.013328190627845335 presición media = 0.8411249995231629\n"
     ]
    }
   ],
   "source": [
    "media = accurasies.mean()\n",
    "varianza = accurasies.std()\n",
    "print(f'Varianza = {varianza} presición media = {media}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis\n",
    "- Tenemos una varianza baja pero una presición un poco alejada del centro, por lo tanto podemos concluir que tenemos poca varianza y alto sesgo. \n",
    "- De la tabla de accuracys podemos ver qua las variaciones obtenidas entre los diferentes entrenamientos los porcentajes no difieren en +/- 2%. Si los valores obtenidos fuesen demasido grandes entre unos y otros, seria necesario añadir a nuestro modelo un metodo de regularización.\n",
    "- Los metodos de regularización son metódos que permiten penalizar mas los datos demasiado alejados de los valores reales. \n",
    "- Uno de ellos es el dropout, el cual 'apaga' un porcentaje de neuronas por cada capa para evitar que todas las neuronas aprendan los mismos patrones, de esta manera se evita el overfit. \n",
    "- El dropout se puede implementar como uno de los metodos de la libreria layers de Keras. La linea que indica cual es la cantidad de neuronas a apagar se implementa inmediatamente despues de la capa donde apagaremos dichas neuronas. Solo requiere que indiquemos el porcentaje de las neuronas que deseamos apagar en cada entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejorando la red - Grid Search\n",
    "Hasta este punto los hiperparametros como: epochs, layers, neuronas por capa, batch_size, loss, optimizer entre otros se han mantenido fijos. Para mejorar los resultados entregados por la red neuronal es necesario variar estos valores. Para no tener que cambiar estos valores a ensayo y error se puede aplicar un metódo denominado grid search. \n",
    "\n",
    "**GridSearchCV:** consiste en fabricar una especie de red, en la que indicamos los posibles valores que deseamos proponer para evaluar en cada hiperparametro. Luego se hace el entrenamiento de nuestra red usando las diferentes combinaciones de valores que tenemos como posibles variaciones. Este metódo implementa el cross validation, de tal manera que usaremos el mismo metódo de funcionamiento. La rutina consta de:\n",
    "\n",
    "1. Especificar el KerasClassifier entregandole como función el modelo que invoca nuestra estructura de red neuronal. Recordar que esta función especifica los argumentos de entrenamiento .fit  \n",
    "2. Los hiper parametros de entrenamiento no los pasaremos como argumentos en el kerasClassifier. Es en este punto donde debemos implementar el grid-search para determinar que combinación es la que mejor resultado nos entrega. \n",
    "3. Como se mensiono debemos crear diferentes valores de los hiperparamentros, para crear este conjunto de datos usamos un diccionario, donde los key seran los nombres de los hiperparamentros y los valores serán sus posibles datos. \n",
    "5. Para implementar el grid usaremos la función GridSearchCV, los argumentos necesarios son:\n",
    "- estimator = classifier, que sera el  KerasClassifier\n",
    "- param_grid = parameters, diccionario con posibles valores para alimentar el grid-search. Se debe cuidar no tener demasiados hiperparametros, pues aumentarian el costo computacional. \n",
    "- scoring = 'accuracy',  es el parametro que deseamos clasificar\n",
    "- cv = 10, es la cantidad de folds que usarmenos\n",
    "6. Para modificar patrones dentro del modelo de red neuronal, debemos agregar como parametro de la función los hiperparametros que deseamos variar mediante el grid search.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la función de red neuronal con el parametro optimizer para variar el optimizador. En este caso: adam y rmsprop. \n",
    "\n",
    "rmsprop es un optimizador común para clasificación en redes neuronale de aprendizaje con pocas capas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = \"uniform\",  activation = \"relu\", input_dim = 11))\n",
    "    model.add(Dropout(0.1)) #Regularización de las neuronas\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = \"uniform\",  activation = \"relu\"))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = \"uniform\",  activation = \"sigmoid\"))\n",
    "    classifier.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la función que nos ayuda a llamar la función modelo en formato wraper. Osea envolviento las librerias de sklearn con keras. Como se ve solo se llama la función sin especificar los hiperparametros de .fit, estos hiperparametros los variaremos mediante el grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasifier = KerasClassifier(build_fn = model_clasificator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por facilidad declararemos los hiperparametros del grid search mediante un diccionario, para luedo entregarlos como parametro de la función GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'batch_size' : [10, 25,32],\n",
    "    'nb_epoch' : [50, 100, 500], \n",
    "    'optimizer' : ['adam', 'rmsprop']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se creara una variable grid_search que guardara el valor de la mejor estimación encontrada al intercambiar los diferentes hiperparametros del grid. La función GridSearchCV recibe los datos:\n",
    "- Estimator = classifier en este caso es nuestro modelo invocado en un grapper de keras y sklearn\n",
    "- param_grid = Diccionario con los hiperparametros de nuestra grid\n",
    "- scoring = Es el valor que objetivo a minimizar mediante nuestro modelo en esta caso la presición.\n",
    "- cv = Es el numero de folds que aplicaremos en el cross validation generalmente un valor entre 5 y 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = classifier, \n",
    "                           param_grid = parameters, \n",
    "                           scoring = 'accuracy', \n",
    "                           cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente vamos a entrenar la red mediante el grid_search.fit y entregamos como es de esperar los datos de entrenamiento de variable independiente y dependiente. \n",
    "\n",
    "Este entrenmiento nos entrega los mejores parametros y la mejor variable objetivo lograda en este caso la presición accuracy. \n",
    "\n",
    "Estos datos se acceden mediante los metodos best_params_ y best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 0s 598us/step - loss: 0.4870 - accuracy: 0.7967\n",
      "720/720 [==============================] - 0s 540us/step - loss: 0.4953 - accuracy: 0.7953\n",
      "720/720 [==============================] - 0s 529us/step - loss: 0.4878 - accuracy: 0.7954\n",
      "720/720 [==============================] - 0s 499us/step - loss: 0.4851 - accuracy: 0.7969\n",
      "720/720 [==============================] - 0s 556us/step - loss: 0.4935 - accuracy: 0.7936\n",
      "720/720 [==============================] - 0s 594us/step - loss: 0.4928 - accuracy: 0.7943\n",
      "720/720 [==============================] - 0s 661us/step - loss: 0.4914 - accuracy: 0.7965\n",
      "720/720 [==============================] - 0s 601us/step - loss: 0.4841 - accuracy: 0.7962\n",
      "720/720 [==============================] - 0s 510us/step - loss: 0.4841 - accuracy: 0.7957\n",
      "720/720 [==============================] - 0s 457us/step - loss: 0.4843 - accuracy: 0.7956\n",
      "720/720 [==============================] - 0s 469us/step - loss: 0.5037 - accuracy: 0.7969\n",
      "720/720 [==============================] - 0s 517us/step - loss: 0.5076 - accuracy: 0.7960\n",
      "720/720 [==============================] - 0s 470us/step - loss: 0.5031 - accuracy: 0.7950\n",
      "720/720 [==============================] - 0s 485us/step - loss: 0.5265 - accuracy: 0.7967\n",
      "720/720 [==============================] - 0s 474us/step - loss: 0.5077 - accuracy: 0.7931\n",
      "720/720 [==============================] - 0s 448us/step - loss: 0.5046 - accuracy: 0.7942\n",
      "720/720 [==============================] - 0s 457us/step - loss: 0.5083 - accuracy: 0.7964\n",
      "720/720 [==============================] - 0s 464us/step - loss: 0.6143 - accuracy: 0.7953\n",
      "720/720 [==============================] - 0s 483us/step - loss: 0.5007 - accuracy: 0.7946\n",
      "  1/720 [..............................] - ETA: 0s - loss: 0.6931 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "720/720 [==============================] - 0s 519us/step - loss: 0.5104 - accuracy: 0.7956\n",
      "720/720 [==============================] - 0s 485us/step - loss: 0.4924 - accuracy: 0.7962\n",
      "720/720 [==============================] - 0s 483us/step - loss: 0.4938 - accuracy: 0.7964\n",
      "720/720 [==============================] - 0s 595us/step - loss: 0.4901 - accuracy: 0.7956\n",
      "720/720 [==============================] - 0s 527us/step - loss: 0.4894 - accuracy: 0.7972\n",
      "720/720 [==============================] - 0s 551us/step - loss: 0.5078 - accuracy: 0.7935\n",
      "720/720 [==============================] - 0s 527us/step - loss: 0.4815 - accuracy: 0.7943\n",
      "720/720 [==============================] - 0s 606us/step - loss: 0.4905 - accuracy: 0.7967\n",
      "720/720 [==============================] - 0s 519us/step - loss: 0.4879 - accuracy: 0.7957\n",
      "720/720 [==============================] - 0s 521us/step - loss: 0.5041 - accuracy: 0.7949\n",
      "720/720 [==============================] - 0s 470us/step - loss: 0.5038 - accuracy: 0.7958\n",
      "720/720 [==============================] - 0s 635us/step - loss: 0.5022 - accuracy: 0.7969\n",
      "720/720 [==============================] - 0s 490us/step - loss: 0.4938 - accuracy: 0.7969\n",
      "720/720 [==============================] - 0s 543us/step - loss: 0.5032 - accuracy: 0.7953\n",
      "  1/720 [..............................] - ETA: 0s - loss: 0.6932 - accuracy: 0.3000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "720/720 [==============================] - 0s 551us/step - loss: 0.4974 - accuracy: 0.7968\n",
      "720/720 [==============================] - 0s 626us/step - loss: 0.5464 - accuracy: 0.7926\n",
      "720/720 [==============================] - 0s 513us/step - loss: 0.5461 - accuracy: 0.7939\n",
      "720/720 [==============================] - 0s 605us/step - loss: 0.5063 - accuracy: 0.7964\n",
      "720/720 [==============================] - 0s 570us/step - loss: 0.5213 - accuracy: 0.7960\n",
      "720/720 [==============================] - 0s 631us/step - loss: 0.5130 - accuracy: 0.7950\n",
      "720/720 [==============================] - 0s 691us/step - loss: 0.5106 - accuracy: 0.7957\n",
      "720/720 [==============================] - 0s 683us/step - loss: 0.4920 - accuracy: 0.7964\n",
      "720/720 [==============================] - 0s 589us/step - loss: 0.5201 - accuracy: 0.7954\n",
      "720/720 [==============================] - 0s 513us/step - loss: 0.4986 - accuracy: 0.7947\n",
      "720/720 [==============================] - 0s 546us/step - loss: 0.4944 - accuracy: 0.7969\n",
      "  1/720 [..............................] - ETA: 0s - loss: 0.6932 - accuracy: 0.4000WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "720/720 [==============================] - 1s 762us/step - loss: 0.4933 - accuracy: 0.7931\n",
      "720/720 [==============================] - 0s 665us/step - loss: 0.5056 - accuracy: 0.7933\n",
      "720/720 [==============================] - 0s 689us/step - loss: 0.4957 - accuracy: 0.7968\n",
      "720/720 [==============================] - 0s 623us/step - loss: 0.4878 - accuracy: 0.7956\n",
      "720/720 [==============================] - 0s 647us/step - loss: 0.4934 - accuracy: 0.7951\n",
      "720/720 [==============================] - 0s 613us/step - loss: 0.4840 - accuracy: 0.7961\n",
      "720/720 [==============================] - 0s 637us/step - loss: 0.5174 - accuracy: 0.7962\n",
      "  1/720 [..............................] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "720/720 [==============================] - 0s 649us/step - loss: 0.5201 - accuracy: 0.7961\n",
      "720/720 [==============================] - 0s 551us/step - loss: 0.5024 - accuracy: 0.7950\n",
      "720/720 [==============================] - 0s 544us/step - loss: 0.5184 - accuracy: 0.7971\n",
      "720/720 [==============================] - 1s 790us/step - loss: 0.4997 - accuracy: 0.7936\n",
      "720/720 [==============================] - 0s 677us/step - loss: 0.5053 - accuracy: 0.7944\n",
      "720/720 [==============================] - 0s 591us/step - loss: 0.5204 - accuracy: 0.7964\n",
      "720/720 [==============================] - 0s 561us/step - loss: 0.5244 - accuracy: 0.7954\n",
      "720/720 [==============================] - 0s 593us/step - loss: 0.6142 - accuracy: 0.7954\n",
      "720/720 [==============================] - 0s 513us/step - loss: 0.5156 - accuracy: 0.7958\n",
      "288/288 [==============================] - 0s 713us/step - loss: 0.5606 - accuracy: 0.7956\n",
      "  1/288 [..............................] - ETA: 0s - loss: 0.6931 - accuracy: 0.5200WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "288/288 [==============================] - 0s 594us/step - loss: 0.5540 - accuracy: 0.7957\n",
      "  1/288 [..............................] - ETA: 0s - loss: 0.6931 - accuracy: 0.7600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "288/288 [==============================] - 0s 501us/step - loss: 0.5595 - accuracy: 0.7953\n",
      "288/288 [==============================] - 0s 472us/step - loss: 0.5513 - accuracy: 0.7976\n",
      "288/288 [==============================] - 0s 568us/step - loss: 0.5796 - accuracy: 0.7929\n",
      "288/288 [==============================] - 0s 495us/step - loss: 0.5845 - accuracy: 0.7931\n",
      "288/288 [==============================] - 0s 564us/step - loss: 0.5635 - accuracy: 0.7969\n",
      "288/288 [==============================] - 0s 512us/step - loss: 0.5507 - accuracy: 0.7957\n",
      "288/288 [==============================] - 0s 490us/step - loss: 0.5551 - accuracy: 0.7954\n",
      "288/288 [==============================] - 0s 500us/step - loss: 0.5622 - accuracy: 0.7950\n",
      "288/288 [==============================] - 0s 551us/step - loss: 0.5702 - accuracy: 0.7962\n",
      "288/288 [==============================] - 0s 558us/step - loss: 0.6000 - accuracy: 0.7961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/288 [==============================] - 0s 512us/step - loss: 0.5733 - accuracy: 0.7947\n",
      "288/288 [==============================] - 0s 628us/step - loss: 0.5677 - accuracy: 0.7967\n",
      "288/288 [==============================] - 0s 554us/step - loss: 0.5797 - accuracy: 0.7932\n",
      "288/288 [==============================] - 0s 535us/step - loss: 0.5643 - accuracy: 0.7944\n",
      "288/288 [==============================] - 0s 509us/step - loss: 0.5757 - accuracy: 0.7968\n",
      "288/288 [==============================] - 0s 554us/step - loss: 0.5870 - accuracy: 0.7949\n",
      "288/288 [==============================] - 0s 540us/step - loss: 0.5722 - accuracy: 0.7951\n",
      "288/288 [==============================] - 0s 519us/step - loss: 0.5685 - accuracy: 0.7951\n",
      "288/288 [==============================] - 0s 509us/step - loss: 0.5555 - accuracy: 0.7957\n",
      "288/288 [==============================] - 0s 455us/step - loss: 0.5788 - accuracy: 0.7943\n",
      "288/288 [==============================] - 0s 545us/step - loss: 0.5735 - accuracy: 0.7939\n",
      "288/288 [==============================] - 0s 607us/step - loss: 0.5467 - accuracy: 0.7967\n",
      "288/288 [==============================] - 0s 498us/step - loss: 0.5587 - accuracy: 0.7924\n",
      "288/288 [==============================] - 0s 500us/step - loss: 0.6043 - accuracy: 0.7931\n",
      "288/288 [==============================] - 0s 530us/step - loss: 0.5583 - accuracy: 0.7951\n",
      "288/288 [==============================] - 0s 681us/step - loss: 0.6000 - accuracy: 0.7951\n",
      "288/288 [==============================] - 0s 609us/step - loss: 0.5553 - accuracy: 0.7956\n",
      "288/288 [==============================] - 0s 599us/step - loss: 0.5847 - accuracy: 0.7943\n",
      "  1/288 [..............................] - ETA: 0s - loss: 0.6931 - accuracy: 0.6000WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "288/288 [==============================] - 0s 644us/step - loss: 0.5859 - accuracy: 0.7965\n",
      "288/288 [==============================] - 0s 565us/step - loss: 0.6081 - accuracy: 0.7961\n",
      "288/288 [==============================] - 0s 559us/step - loss: 0.6006 - accuracy: 0.7940\n",
      "288/288 [==============================] - 0s 689us/step - loss: 0.5671 - accuracy: 0.7971\n",
      "288/288 [==============================] - 0s 628us/step - loss: 0.5640 - accuracy: 0.7932\n",
      "288/288 [==============================] - 0s 502us/step - loss: 0.6009 - accuracy: 0.7935\n",
      "  1/288 [..............................] - ETA: 0s - loss: 0.6931 - accuracy: 0.6000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "288/288 [==============================] - 0s 523us/step - loss: 0.6083 - accuracy: 0.7964\n",
      "288/288 [==============================] - 0s 565us/step - loss: 0.5874 - accuracy: 0.7950\n",
      "288/288 [==============================] - 0s 613us/step - loss: 0.5761 - accuracy: 0.7954\n",
      "288/288 [==============================] - 0s 494us/step - loss: 0.5549 - accuracy: 0.7958\n",
      "288/288 [==============================] - 0s 523us/step - loss: 0.5587 - accuracy: 0.7971\n",
      "288/288 [==============================] - 0s 533us/step - loss: 0.5831 - accuracy: 0.7937\n",
      "288/288 [==============================] - 0s 479us/step - loss: 0.5689 - accuracy: 0.7940\n",
      "288/288 [==============================] - 0s 469us/step - loss: 0.5560 - accuracy: 0.7967\n",
      "288/288 [==============================] - 0s 592us/step - loss: 0.5579 - accuracy: 0.7926\n",
      "288/288 [==============================] - 0s 474us/step - loss: 0.5485 - accuracy: 0.7940\n",
      "288/288 [==============================] - 0s 568us/step - loss: 0.5439 - accuracy: 0.7969\n",
      "288/288 [==============================] - 0s 565us/step - loss: 0.5678 - accuracy: 0.7950\n",
      "288/288 [==============================] - 0s 482us/step - loss: 0.5708 - accuracy: 0.7943\n",
      "288/288 [==============================] - 0s 522us/step - loss: 0.5784 - accuracy: 0.7954\n",
      "288/288 [==============================] - 0s 494us/step - loss: 0.5870 - accuracy: 0.7951\n",
      "288/288 [==============================] - 0s 496us/step - loss: 0.6030 - accuracy: 0.7949\n",
      "288/288 [==============================] - 0s 716us/step - loss: 0.5959 - accuracy: 0.7951\n",
      "288/288 [==============================] - 0s 573us/step - loss: 0.6345 - accuracy: 0.7967\n",
      "288/288 [==============================] - 0s 536us/step - loss: 0.5938 - accuracy: 0.7925\n",
      "288/288 [==============================] - 0s 507us/step - loss: 0.6540 - accuracy: 0.7926\n",
      "288/288 [==============================] - 0s 621us/step - loss: 0.5568 - accuracy: 0.7968\n",
      "288/288 [==============================] - 0s 703us/step - loss: 0.5714 - accuracy: 0.7964\n",
      "288/288 [==============================] - 0s 751us/step - loss: 0.5874 - accuracy: 0.7944\n",
      "288/288 [==============================] - 0s 534us/step - loss: 0.5811 - accuracy: 0.7949\n",
      "225/225 [==============================] - 0s 671us/step - loss: 0.6089 - accuracy: 0.7951\n",
      "225/225 [==============================] - 0s 526us/step - loss: 0.5980 - accuracy: 0.7950\n",
      "225/225 [==============================] - 0s 527us/step - loss: 0.5987 - accuracy: 0.7942\n",
      "225/225 [==============================] - 0s 749us/step - loss: 0.6041 - accuracy: 0.7964\n",
      "225/225 [==============================] - 0s 869us/step - loss: 0.5775 - accuracy: 0.7928\n",
      "225/225 [==============================] - 0s 770us/step - loss: 0.5994 - accuracy: 0.7936\n",
      "225/225 [==============================] - 0s 694us/step - loss: 0.6076 - accuracy: 0.7954\n",
      "225/225 [==============================] - 0s 467us/step - loss: 0.5784 - accuracy: 0.7953\n",
      "225/225 [==============================] - 0s 563us/step - loss: 0.6005 - accuracy: 0.7940\n",
      "225/225 [==============================] - 0s 741us/step - loss: 0.5818 - accuracy: 0.7947\n",
      "225/225 [==============================] - 0s 723us/step - loss: 0.5980 - accuracy: 0.7969\n",
      "225/225 [==============================] - 0s 503us/step - loss: 0.6014 - accuracy: 0.7961\n",
      "  1/225 [..............................] - ETA: 0s - loss: 0.6931 - accuracy: 0.6562WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "225/225 [==============================] - 0s 483us/step - loss: 0.6018 - accuracy: 0.7951\n",
      "225/225 [==============================] - 0s 565us/step - loss: 0.6010 - accuracy: 0.7964\n",
      "225/225 [==============================] - 0s 533us/step - loss: 0.6164 - accuracy: 0.7908\n",
      "225/225 [==============================] - 0s 446us/step - loss: 0.6031 - accuracy: 0.7931\n",
      "225/225 [==============================] - 0s 445us/step - loss: 0.5859 - accuracy: 0.7968\n",
      "225/225 [==============================] - 0s 461us/step - loss: 0.5990 - accuracy: 0.7960\n",
      "225/225 [==============================] - 0s 489us/step - loss: 0.6110 - accuracy: 0.7944\n",
      "225/225 [==============================] - 0s 547us/step - loss: 0.6120 - accuracy: 0.7942\n",
      "225/225 [==============================] - 0s 489us/step - loss: 0.5893 - accuracy: 0.7965\n",
      "225/225 [==============================] - 0s 531us/step - loss: 0.6107 - accuracy: 0.7947\n",
      "225/225 [==============================] - 0s 484us/step - loss: 0.6052 - accuracy: 0.7932\n",
      "225/225 [==============================] - 0s 502us/step - loss: 0.5826 - accuracy: 0.7964\n",
      "225/225 [==============================] - 0s 526us/step - loss: 0.5967 - accuracy: 0.7911\n",
      "225/225 [==============================] - 0s 525us/step - loss: 0.5917 - accuracy: 0.7917\n",
      "225/225 [==============================] - 0s 542us/step - loss: 0.5879 - accuracy: 0.7954\n",
      "225/225 [==============================] - 0s 497us/step - loss: 0.6160 - accuracy: 0.7939\n",
      "225/225 [==============================] - 0s 559us/step - loss: 0.5880 - accuracy: 0.7954\n",
      "225/225 [==============================] - 0s 497us/step - loss: 0.5744 - accuracy: 0.7961\n",
      "225/225 [==============================] - 0s 523us/step - loss: 0.6168 - accuracy: 0.7954\n",
      "225/225 [==============================] - 0s 556us/step - loss: 0.6014 - accuracy: 0.7960\n",
      "225/225 [==============================] - 0s 486us/step - loss: 0.6480 - accuracy: 0.7928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 0s 589us/step - loss: 0.5939 - accuracy: 0.7954\n",
      "225/225 [==============================] - 0s 630us/step - loss: 0.5979 - accuracy: 0.7904\n",
      "225/225 [==============================] - 0s 512us/step - loss: 0.5967 - accuracy: 0.7936\n",
      "225/225 [==============================] - 0s 570us/step - loss: 0.6005 - accuracy: 0.7975\n",
      "225/225 [==============================] - 0s 486us/step - loss: 0.5884 - accuracy: 0.7958\n",
      "225/225 [==============================] - 0s 446us/step - loss: 0.5933 - accuracy: 0.7946\n",
      "225/225 [==============================] - 0s 452us/step - loss: 0.5937 - accuracy: 0.7949\n",
      "225/225 [==============================] - 0s 524us/step - loss: 0.5922 - accuracy: 0.7962\n",
      "225/225 [==============================] - 0s 585us/step - loss: 0.5917 - accuracy: 0.7961\n",
      "225/225 [==============================] - 0s 555us/step - loss: 0.5991 - accuracy: 0.7943\n",
      "225/225 [==============================] - 0s 580us/step - loss: 0.5732 - accuracy: 0.7974\n",
      "225/225 [==============================] - 0s 491us/step - loss: 0.5866 - accuracy: 0.7928\n",
      "225/225 [==============================] - 0s 443us/step - loss: 0.5916 - accuracy: 0.7937\n",
      "225/225 [==============================] - 0s 515us/step - loss: 0.6098 - accuracy: 0.7964\n",
      "225/225 [==============================] - 0s 609us/step - loss: 0.5729 - accuracy: 0.7962\n",
      "225/225 [==============================] - 0s 442us/step - loss: 0.5977 - accuracy: 0.7931\n",
      "225/225 [==============================] - 0s 727us/step - loss: 0.5878 - accuracy: 0.7947\n",
      "225/225 [==============================] - 0s 486us/step - loss: 0.5879 - accuracy: 0.7969\n",
      "225/225 [==============================] - 0s 518us/step - loss: 0.5981 - accuracy: 0.7957\n",
      "225/225 [==============================] - 0s 486us/step - loss: 0.6265 - accuracy: 0.7929\n",
      "225/225 [==============================] - 0s 445us/step - loss: 0.5964 - accuracy: 0.7964\n",
      "225/225 [==============================] - 0s 547us/step - loss: 0.6408 - accuracy: 0.7918\n",
      "225/225 [==============================] - 0s 612us/step - loss: 0.6033 - accuracy: 0.7935\n",
      "  1/225 [..............................] - ETA: 0s - loss: 0.6932 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "225/225 [==============================] - 0s 507us/step - loss: 0.6264 - accuracy: 0.7953\n",
      "225/225 [==============================] - 0s 514us/step - loss: 0.6227 - accuracy: 0.7946\n",
      "225/225 [==============================] - 0s 585us/step - loss: 0.6139 - accuracy: 0.7931\n",
      "225/225 [==============================] - 0s 599us/step - loss: 0.6022 - accuracy: 0.7954\n",
      "800/800 [==============================] - 0s 606us/step - loss: 0.4951 - accuracy: 0.7949\n"
     ]
    }
   ],
   "source": [
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parametros {'batch_size': 10, 'nb_epoch': 500, 'optimizer': 'adam'}\n",
      "Mejor presición accuracy obtenida 0.79625\n"
     ]
    }
   ],
   "source": [
    "print(f'Mejores parametros {best_parameters}')\n",
    "print(f'Mejor presición accuracy obtenida {best_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
